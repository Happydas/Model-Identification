{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cda1fc8d5618>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpdefind\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mPDE_Equation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpde_matrix_mul\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_coeff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_xi_threshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpde_Recover\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'scale_xi_threshold' from 'PDE_Equation' (D:\\Thesis\\Neural Network\\Project\\Model Identification\\PDE_Equation.py)"
     ],
     "ename": "ImportError",
     "evalue": "cannot import name 'scale_xi_threshold' from 'PDE_Equation' (D:\\Thesis\\Neural Network\\Project\\Model Identification\\PDE_Equation.py)",
     "output_type": "error"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from torch import tensor\n",
    "\n",
    "from torch.autograd import grad\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from pdefind import *\n",
    "from PDE_Equation import pde_matrix_mul, sparse_coeff, scale_xi_threshold, pde_Recover\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "polynm = ['1', 'u', 'uË†2']\n",
    "spa_der = ['1', 'u_{x}', 'u_{xx}']\n",
    "library_coeffs = pde_matrix_mul(polynm, spa_der)\n",
    "print('library_coeffs:', library_coeffs)\n",
    "\n",
    "tot_items = len(library_coeffs)\n",
    "print('tot_items:', tot_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Data Setup\n",
    "data = sio.loadmat(os.path.join(os.getcwd(), \"data\", \"burgers.mat\"))\n",
    "u = data[\"usol\"]\n",
    "x = data[\"x\"][0]\n",
    "t = np.squeeze(data[\"t\"], axis=1)\n",
    "\n",
    "print(\"u shape\", u.shape)\n",
    "print(\"x shape\", x.shape)\n",
    "print(\"t shape\", t.shape)\n",
    "\n",
    "im = plt.imshow(u.real, cmap=\"seismic\", aspect=0.4)\n",
    "plt.colorbar(im)\n",
    "plt.title(\"Burgers Equation\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"x\")\n",
    "\n",
    "xpos  = np.arange(len(t), step=25)\n",
    "ypos  = np.where(x%4==0)[0].tolist()\n",
    "ytick = x[ypos].tolist()\n",
    "ypos  += [len(x)-1]\n",
    "ytick += [8.0]\n",
    "\n",
    "plt.xticks(xpos, t[xpos])\n",
    "plt.yticks(ypos, ytick)\n",
    "plt.gcf().set_size_inches(6,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Add Noise\n",
    "np.random.seed(0)\n",
    "noise_level = 0.1\n",
    "u = u.real + noise_level*np.std(u.real)*np.random.randn(u.shape[0],u.shape[1])\n",
    "\n",
    "im = plt.imshow(u.real, cmap=\"seismic\", aspect=0.4)\n",
    "plt.colorbar(im)\n",
    "plt.title(\"Noisy Data\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"x\")\n",
    "\n",
    "plt.xticks(xpos, t[xpos])\n",
    "plt.yticks(ypos, ytick)\n",
    "plt.gcf().set_size_inches(6,5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Prepare Training Data\n",
    "xx, tt = np.meshgrid(x,t)\n",
    "X = np.vstack([xx.ravel(), tt.ravel()]).T\n",
    "print(\"X shape\", X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "y = np.zeros((u.size, 1), dtype=np.float)\n",
    "for i,_x in enumerate(u.real.T):\n",
    "    y[i*len(x):(i+1)*len(x)] = _x.reshape(len(x),1)\n",
    "    \n",
    "print(\"y shape\", y.shape)\n",
    "\n",
    "plt.plot(y[:256], label=\"t=0\")\n",
    "plt.plot(y[50*256:51*256], label=\"t=50\")\n",
    "plt.plot(y[100*256:101*256], label=\"t=100\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"u\")\n",
    "plt.xticks(ypos, ytick)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idxs = np.random.choice(y.size, 1000, replace=False)\n",
    "\n",
    "X_train = torch.tensor(X[idxs], dtype=torch.float32, requires_grad=True)\n",
    "y_train = torch.tensor(y[idxs], dtype=torch.float32)\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"X shape\", X.shape)\n",
    "print(\"y shape\", y.shape)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "im = plt.imshow(u, cmap=\"seismic\", aspect=0.4)\n",
    "plt.colorbar(im)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"x\")\n",
    "plt.title(\"Noisy\")\n",
    "\n",
    "plt.xticks(xpos, t[xpos])\n",
    "plt.yticks(ypos, ytick)\n",
    "\n",
    "usamp = np.full(y.shape,np.nan)\n",
    "usamp[idxs] = y[idxs]\n",
    "usamp = usamp.reshape(u.T.shape).T\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "im = plt.imshow(usamp, cmap=\"seismic\", aspect=.4)\n",
    "plt.colorbar(im)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"x\")\n",
    "plt.title(\"Sampled\")\n",
    "\n",
    "plt.xticks(xpos, t[xpos])\n",
    "plt.yticks(ypos, ytick)\n",
    "\n",
    "plt.gcf().set_size_inches(10,4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Setup the network\n",
    "net = PINN(sizes=[2,20,15,10,5,1], activation=torch.nn.Tanh())\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Building Library\n",
    "def build_library(uhat, data, poly_order, deriv_order):\n",
    "    # build polynomials\n",
    "    poly = torch.ones_like(uhat)\n",
    "    \n",
    "    # concatinate different orders\n",
    "    for o in np.arange(1, poly_order+1):\n",
    "        poly_o = poly[:,o-1:o]*uhat\n",
    "        #print('poly_o:',poly_o)\n",
    "        poly = torch.cat((poly, poly_o), dim=1)\n",
    "        \n",
    "    # build derivatives\n",
    "    # returns gradient of uhat w.r.t. data (id0=spatial, id1=temporal)\n",
    "    du = grad(outputs=uhat, inputs=data, \n",
    "              grad_outputs=torch.ones_like(uhat), create_graph=True)[0]\n",
    "    \n",
    "    # time derivative\n",
    "    dudt = du[:,1:2]\n",
    "    \n",
    "    # spatial derivatives\n",
    "    dudx = torch.cat((torch.ones_like(dudt), du[:,0:1]), dim=1)\n",
    "    \n",
    "    # concatinate different orders\n",
    "    for o in np.arange(1, deriv_order):\n",
    "        du = grad(outputs=dudx[:,o:o+1], inputs=data, \n",
    "                  grad_outputs=torch.ones_like(uhat), create_graph=True)[0]\n",
    "        dudx = torch.cat((dudx, du[:,0:1]), dim=1)\n",
    "    \n",
    "    # build all possible combinations of poly and dudx vectors\n",
    "    theta = None\n",
    "    for i in range(poly_order+1):\n",
    "        for j in range(deriv_order+1):\n",
    "            comb = poly[:,i:i+1] * dudx[:,j:j+1]\n",
    "            \n",
    "            if theta is None:\n",
    "                theta = comb\n",
    "            else:\n",
    "                theta = torch.cat((theta, comb), dim=1)\n",
    "                \n",
    "    return dudt, theta\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "epochs = 4000\n",
    "#xi = nn.Parameter(torch.randn((1, 1), requires_grad=True, device=\"cpu\", dtype=torch.float32))\n",
    "xi = nn.Parameter(torch.randn((tot_items, 1), requires_grad=True, device=\"cpu\", dtype=torch.float32))\n",
    "#xi = torch.tensor([[0.1], [-1]])\n",
    "#print(xi)\n",
    "#params = [{'params': net.parameters(), 'lr': 1e-3}]\n",
    "params = [{'params': net.parameters(), 'lr': 1e-3}, {'params': xi, 'lr': 2e-2}]\n",
    "\n",
    "optimizer = Adam(params)\n",
    "scheduler = ExponentialLR(optimizer, .9998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def PI_NeuralNet(uhat, X_train, mask, poly_order, deriv_order):\n",
    "    lamb   = 0\n",
    "    tolerance = 10**-6\n",
    "    mask = torch.ones(tot_items, 1)\n",
    "    print('xi', xi)\n",
    "    print('mask:', mask.shape)\n",
    "    \n",
    "    dudt, theta = build_library(uhat, X_train, poly_order=2, deriv_order=2)\n",
    "\n",
    "    dudt_norm = torch.norm(dudt, dim=0)\n",
    "    print('dudt_norm:', dudt_norm.shape)\n",
    "    \n",
    "   \n",
    "    theta_scaling = (torch.norm(theta, dim=0))\n",
    "    #print('theta_scaling:', theta_scaling.shape)\n",
    "    scaling_theta = torch.unsqueeze(theta_scaling, dim = 1)\n",
    "    theta_norm = torch.unsqueeze(theta_scaling, dim = 1) \n",
    "    print('theta_norm:', theta_norm.shape)\n",
    "    #print('xi:', xi.shape)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        uhat = net(X_train)\n",
    "    \n",
    "        if epoch == 1000:\n",
    "            lamb = 1\n",
    "               \n",
    "        dudt, theta = build_library(uhat, X_train, poly_order=2, deriv_order=2)\n",
    "        #print('dudt:', dudt.shape)\n",
    "        #theta = theta[:, [2,4]].clone()\n",
    "        l_u   = nn.MSELoss()(uhat, y_train)\n",
    "        l_reg = lamb * torch.mean((dudt - theta @ xi)**2)\n",
    "\n",
    "        loss = l_u + l_reg\n",
    "        #print('loss', loss)\n",
    "    \n",
    "\n",
    "        xi_normalized = xi * (theta_norm / dudt_norm) \n",
    "        #print('xi_normalized:', xi_normalized.shape)\n",
    "\n",
    "\n",
    "        #gradient_losses = torch.max(torch.abs(grad(outputs=loss, inputs=xi, \n",
    "              #grad_outputs=torch.ones_like(loss), create_graph=True)[0]))\n",
    "        \n",
    "        gradient_losses = torch.max(torch.abs(grad(outputs=loss, inputs=xi, \n",
    "              grad_outputs=torch.ones_like(loss), create_graph=True)[0]) / (theta_norm / dudt_norm))\n",
    "        \n",
    "        \n",
    "        gradient_loss = torch.max(gradient_losses)\n",
    "        \n",
    "        #gradient_losses = torch.abs(grad(outputs=loss, inputs=xi, create_graph=True)[0])\n",
    "        #print('gradient_losses:', gradient_losses)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        #print(\"epoch {}/{}, loss={:.10f}\".format(epoch+1, epochs, loss.item()), end=\"\\r\")\n",
    "        \n",
    "        if epoch % 1500 == 0:\n",
    "            #if epoch % 500 == 0:\n",
    "            print('gradient_loss:', gradient_loss)\n",
    "            print('loss:', loss)\n",
    "            #if (gradient_losses) < (tolerance):\n",
    "            if gradient_loss < tolerance:\n",
    "                print('Optimizer converged.')\n",
    "                #break\n",
    "    #print('xi', xi)    \n",
    "    #print('xi_normalized:', xi_normalized)\n",
    "    xi_list = sparse_coeff(mask, xi.detach().numpy())\n",
    "    #print('xi_list:', xi_list)\n",
    "    xi_normalized = sparse_coeff(mask, xi_normalized.detach().numpy())\n",
    "    #print('xi_normalized:', xi_normalized)\n",
    "    print('Finished')\n",
    "            \n",
    "            \n",
    "    return xi_list, xi_normalized      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#mask = torch.ones(9, 1)\n",
    "mask = torch.ones(tot_items, 1)\n",
    "#xi =torch.randn((tot_items, 1), dtype=torch.float32, requires_grad = True)\n",
    "uhat = net(X_train)\n",
    "xi_list, xi_normalized= PI_NeuralNet(uhat, X_train, mask, poly_order=2, deriv_order=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def model_Identification(data, uhat, mask, tot_items, poly_order, deriv_order):\n",
    "    #mask = torch.ones(9, 1)\n",
    "    mask = torch.ones(tot_items, 1)\n",
    "    xi_list, xi_normalized = PI_NeuralNet(uhat, X_train, mask, poly_order, deriv_order)\n",
    "    sparsity = scale_xi_threshold( xi_normalized, mode='auto')\n",
    "    #print('sparsity:', sparsity)\n",
    "    \n",
    "    mask[~np.transpose(np.squeeze(np.array(sparsity)))] = 0\n",
    "    xi_thresholded = np.expand_dims(xi_list[sparsity], axis=1) \n",
    "    \n",
    "    # Printing current sparse vector to see progress\n",
    "    print('Coefficient xi:')\n",
    "    print(sparse_coeff(sparsity, xi_thresholded))\n",
    "    \n",
    "    # Now thats it's converged, fit again but without the L1 penalty\n",
    "    print('Now running for the final time...')\n",
    "    \n",
    "    xi, _ = PI_NeuralNet(uhat, X_train, mask, poly_order, deriv_order)\n",
    "    \n",
    "    uhat = net(torch.FloatTensor(data))\n",
    "    xi_list = sparse_coeff(sparsity, xi_thresholded)\n",
    "    #xi_list = xi\n",
    "    print('Finished')\n",
    "    \n",
    "    return xi_list, uhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "xi_list, uhat = model_Identification(X, uhat, mask, tot_items, poly_order=2, deriv_order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(uhat.shape)\n",
    "pde_Recover(xi_list, library_coeffs, equation_form='u_t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "uhat = net(torch.FloatTensor(X))\n",
    "print(\"MSE loss\", nn.MSELoss()(uhat, torch.FloatTensor(y)).item())\n",
    "\n",
    "uhat = uhat.data.reshape(u.T.shape)\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "im = plt.imshow(data[\"usol\"].real, cmap=\"seismic\", aspect=0.4)\n",
    "plt.colorbar(im)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"x\")\n",
    "plt.title(\"Original\")\n",
    "\n",
    "plt.xticks(xpos, t[xpos])\n",
    "plt.yticks(ypos, ytick)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "im = plt.imshow(u, cmap=\"seismic\", aspect=0.4)\n",
    "plt.colorbar(im)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"x\")\n",
    "plt.title(\"Noisy\")\n",
    "\n",
    "plt.xticks(xpos, t[xpos])\n",
    "plt.yticks(ypos, ytick)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "im = plt.imshow(uhat.numpy().T, cmap=\"seismic\", aspect=.4)\n",
    "plt.colorbar(im)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"x\")\n",
    "plt.title(\"Reconstructed\")\n",
    "\n",
    "plt.xticks(xpos, t[xpos])\n",
    "plt.yticks(ypos, ytick)\n",
    "\n",
    "plt.gcf().set_size_inches(18,4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.abs(uhat.numpy().T.data - data[\"usol\"].real), cmap=\"seismic\", aspect=.4)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Denoising with coefficient\n",
    "net = PINN(sizes=[2,20,15,10,5,1], activation=torch.nn.Tanh())\n",
    "print(net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}